# Gameformer:Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving

一种用于自动驾驶的基于博弈论和Transformer的交互预测与规划模型。

核心在于构建了一个编码解码器结构，是一个分层的架构，每一层的输入为上一层的输出，用于优化交互处理结果。

## 背景
当前的encoder对于障碍物之间预测轨迹交互问题以及障碍物与自车轨迹交互问题，建模设计不太良好。也就是说自车规控模块与障碍物轨迹预测难以进行较好的交互反馈。本文提出的方法，结合了博弈论和Transformer，能够在进行交互预测的同时，考虑到不同交通参与者的行为和决策。

创新点：

1.分层预测决策一体化框架用于优化目标轨迹交互策略

2.在Waymo数据集上取得了SOTA效果

3.在Waymo, nuPlan上进行了开环闭环仿真验证了实验效果。

> Waymo是一个多智能体互动预测和行为规划的真实世界数据集。特点是多模态，交互式预测任务（一对或多对交通参与者的未来轨迹），提供高精度的标注，包含车辆的位置、速度、轨迹等。

>SOTA（State of the Art）意为当前领域中表现最好的技术或方法。

文章提出了k层的交互decoder框架，基于k-1层的预测结果，k层对上述结果进行交互融合，实现目标预测。
0级智能体自己独立行动，不考虑其他的智能体。1级agent将其他agent当0级来考虑，并预测他们的操作。也就是说k级agent把其他人都当成k-1级，进行预测和响应，符合k级博弈论。
0 级解码器仅使用初始模态查询和编码场景上下文作为键和值来预测代理的多模态未来轨迹。然后，在每次迭代 k 时，k 级解码器将（k−1）级解码器的预测轨迹以及背景信息作为输入，以预测当前级别的智能体轨迹。

## 相关工作
### 轨迹预测
以往的大多数基于Transformer都聚焦于编码Motion Transformer使用迭代局部运动细化来提高预测精准度，受此和分层博弈论的启发，本文提出了基于Transformer解码器的交互预测。
以前的工作，会忽略自动驾驶汽车本身对整个交通场景的影响。（少考虑了自己对他人的影响）。后续又有工作把AV的规划信息，来预测其他智能体的反应，但还是体现的单向交互（没有考虑到其他智能体的行为对AV反馈的影响）。
本文就是旨在解决单向交互问题，通过迭代交互建模来促进AV的规划。
### 决策
离线学习方法可以从大规模驾驶数据集中学习决策策略（利用历史数据，不需要实时与环境交互）；模仿学习可以复制专家行为的驾驶策略；离线强化学习结合了强化学习和收集的大型数集的优点（强化学习：在与环境的交互中优化决策策略；离线：使用已经收集的数据，避免了实时交互带来的高昂成本）；直接政策学习缺乏可解释性（黑箱），安全性和分配变化（训练数据变化会剧烈影响系统）。

文章采用增强运动预测模型。相较于直接政策学习，使用的学习的运动预测。这种模型可解释性好（通过考虑交通参与者和场景场景上下文预测），鲁棒性强。

## GameFormer
![Gameformer框架图](Gameformer_frame.png "Gameformer框架图")
这个框架图主要分为场景上下文编码、Transformer编码器、编码器部分、迭代过程和高斯混合模型。

### 场景上下文
输入的是交通场景，包含交通参与者（其他车辆、行人等）的状态信息和地图信息。
#### Agent State Encoder
对每个交通参与者的历史状态进行编码。
#### Map Polyline Encoder
对地图元素进行编码，帮助模型理解环境。

### Transformer编码器
对场景和参与者的状态进行处理，提取出一个全局的场景上下文表示，包含二者的综合表述。

### 解码器部分
#### level_0解码器
使用Transformer的输出和特定的Query（例如Agent history query 和 Modality embedding query）
Level-0解码器独立地预测每个智能体的未来轨迹，不考虑其他智能体的行为，进行初步的预测。
最后生成每个智能体未来轨迹的高斯混合模型（GMM）预测结果，并计算这些轨迹的得分。

#### level_k解码器
在level_0基础上迭代，使用level_(k-1)的未来预测结果作为输入，结合场景上下文信息。
交叉注意力：level_(k-1)与当前场景上下文信息结合
自注意力：对来自其他智能体的预测进行进一步处理，捕捉各智能体之间的互动。
>交叉注意力的Q和KV来自不同的序列，自注意力的QKV来自同一个序列。

>这里的交叉注意力的Q来自level_k解码器，K和V来自当前场景和leve_(k-1)的预测的拼接。自注意力全部来自level_k。

### 输出
最后输出轨迹预测、分数。用混合高斯预测表示。
>这里的混合高斯的输出，可以看成自车前方出现很多条路径，每条路径并不是一条细线，而是有些许模糊的雾状。

## 方法部分
### Game-theoretic Formulation
联合预测相邻智能体 $Y_{1:N-1}^{1:T_f}$即$1$到$T_f$的时间步下$1$到$N$的智能体的轨迹。
对于某个$i$智能体，其$1$到$T_f$的轨迹预测可如下计算$Y_i^{1:T_f} = \{y_j^{1:T_f},p_j | j=1:M\} $，其中$p_j$是轨迹概率，$M$是模态数量。
将$\pi_i^{(k)}$看作智能体$i$在推理级别$k$的预测多模态轨迹（GMM）。则优化目标为$\min_{\pi_i}L_i^k(\pi_i^{(k)}|\pi_{\neg i}^{(k-1)})$，意思为最小化以其他智能体在level(k-1)的预测轨迹为条件下的第$i$个智能体在level_k的预测的混合高斯的损失函数。
### 场景编码
#### 输入
包含两个一个是历史状态信息，一个是局部矢量化地图
历史状态信息$S_p \in \mathbb{R}^{N \times T_h \times d_s}$，$d_s$为状态属性的数量。局部矢量化地图折线$M \in \mathbb{R}^{N \times N_m \times N_p \times d_p}$，代表对于每个智能体找到$N_m$个附近的地图元素，每个元素包含具有$d_p$属性的$N_p$个航路点。
>举个通俗易懂的例子，假设：一个场景中有 4辆车（$N$=4，每辆车取周围6条车道线（$N_m=6$），每条车道线由10个点组成（$N_p=10$），每个点用 8维向量描述特征（$d_p=8$）。即对于每辆车，模型看到它附近的 6 条车道，每条车道是由10个点（polyline）组成的，每个点是一个 8 维向量。 

#### 智能体历史编码
使用LSTM对每个智能体的历史状态序列$S_p$进行编码，得到一个张量$A_p \in \mathbb{R}^{N \times D}$，包含所有智能体的过去特征。$D$为隐藏特征维度。

#### 地图编码
在矢量化地图折线$M \in \mathbb{R}^{N \times N_m \times N_p \times d_p}$基础上进一步使用最大池化聚合。将每条polyline，将其内部所有点的向量通过max pooling聚合为一个向量，的到$M_r \in \mathbb{R}^{N \times N_{mr} \times D}$。
>$N_{mr}$为每个智能体的地图元素数量。

#### 关系编码
将代理特征及相应的局部地图特征连接起来，得到$C^i = [A^p,M_i^p] \in \mathbb{R}^{(N+N_{mr})\times D}$。再将其喂给一个Transformer编码器，捕捉agent和地图、agent之间的关系。最终生成场景上下文编码$C^s \in \mathbb{R}^{N+N_{mr}\times D}$。

### Level-k 推理的未来解码器
#### 模态嵌入
一个张量$I \in \mathbb{R}^{N \times M \times D}$。用于生成一个可学习的初始模态嵌入张量，其中$M$表示未来模态的数量。

#### level_0解码器
使用多头交叉注意力Transformer模块。
输入为初始模态嵌入$I$和历史信息+场景上下文（$A_p$和$C_s$）
>$A_p$是每个agent的历史编码，表示“我过去是怎么走的”
$C_s$是由场景编码器的到的”“
