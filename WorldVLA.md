# WorldVLA:世界模型实现视觉-动作双向增强，抓取精度显著提升
## 简要介绍
WorldVLA是一种将动作与图像理解和生成相结合的自回归动作世界模型。集成了VLA和世界模型。通过动作和图像理解来预测未来图像，旨在学习环境的底层物理规律来改进动作生成。动作模型基于图像观察生成后续动作，辅助视觉理解，帮助世界模型的视觉生成。

本文的WorldVLA，二者结合的性能较单独的动作模型和世界模型要更优，突显出世界模型与动作模型之间的相互增强的作用。

以自回归方式生成动作序列时，动作模型的性能会下降，归因于模型泛化能力有限，导致早期动作的误差传播到后续动作。本文采用注意力掩码策略，在生成当前动作时选择性地掩码先前动作，有了显著性的性能提升。

![](https://pic2.zhimg.com/v2-594de454510707939eb7c508d4d147c7_r.jpg)
本文的方法实现了前两个的结合。较动作模型多了对图像的生成和行为的理解，较世界模型多了动作的生成。

 自回归顺序生成动作会导致性能下降。这是预训练的多模态语言模型主要是以图像和文本，导致应用在生成动作上时，泛化能力弱。使用选择性的掩码策略，掩盖住一部分先前动作，减少误差累积，在动作块的生成任务中产生了显著的性能提升。

`动作模型负责基于图像观察历史和语言指令生成动作;世界模型基于观察历史序列和相应的动作序列预测下一帧 `

 ## 主要贡献
 一个模型： WorldVLA，将动作和图像理解与生成相结合的自回归动作世界模型
 
 一种掩码策略：为自回归生成动作块时，加入一种动作注意力掩码策略，解决了顺序生成多个动作时的误差累积

 优异的实验结果：优于独立的动作与世界模型；掩码策略解决了性能退化问题，并显著提高了抓取性能。

 ## 实验方法
 ![](https://pic4.zhimg.com/v2-17f1ff7ba0b4f33a7a11a8aeb5fc81c3_1440w.jpg)
 该模型的主要结构如上，分为两大部分。动作模型接受任务指令和当前及历史观察状态，生成动作块。将动作块的输出以及规定好的语言指令以及当前及历史观察状态输入进世界模型，生成未来图像帧。

 ### 涉及三个tokenizer
 图像tokenizer：是一个VQ-GAN模型，带有对特定图像区域（面部和显著对象）的额外感知损失。压缩比为16，codebook大小为8192。

 `对特定图像区域的额外感知损失，可以加强对关键区域的还原质量`

 动作tokenizer：使用分箱操作。每个维度都分成256箱，箱宽度由训练数据范围决定。一个动作包含7个token，有3个性相对位置、3个相对角度和1个绝对抓取器状态。

 语言tokenizer：采用训练过的BPE（Byte Pair Encoding，可合并高频词，提高泛化能力，编码速度快，词表大小可指定）词汇量65536，包含8192个图像token和256个动作token。

 ### 训练策略
 引入世界模型增强动作生成，主要原因有：

 世界模型从当前状态和应用的动作预测未来的图像，来获得对环境的物理解释，对后续操纵任务有帮助；

 世界模型使系统能够模拟和评估动作的潜在结果，有利于避免可能导致不利状态的动作；

 世界模型需要对动作的输入做出精确解释，支持了动作模型可以产生更有效和符合上下文的动作。

 动作模型通过以上增强了视觉理解，进而支持了世界模型的视觉生成能力。

 ### 两个模型的输入输出数据结构
 #### 动作模型
 包含起始、结束、分割的
