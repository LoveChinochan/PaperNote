# DriveMoE
2025年5月的一篇文章，基于MoE的端到端自动驾驶框架。（添加了一个视觉MoE），训练一个路由选择器根据驾驶情境动态选择相关的视觉。因为人在驾驶的时候是选择性关注关键的视觉线索，而不是处理所有视觉信息。在Action部分，训练另一个路由选择器，来激活针对不同驾驶行为的专家模块。

## 传统VLA缺点
![](https://picx.zhimg.com/v2-09c798234bc8da6b608ad9a3faedf949_r.jpg)
现有的VLA有两种不同的多视角输入策略，第一种是普通视觉处理器，在每个时间步不加区分地处理所有可用的相机视图，会有大量的计算负担和冗余的视觉表示；第二种是基于查询的视觉处理器，使用学习的查询（如Q-former模块）来提取由语义上下文引导的一组紧凑的视觉token（只提取出我们感兴趣的部分，但是会丢失几何和位置信息，且需要大量的额外预训练）。

现有的VLA采样单一的统一策略网络设计，用于处理整个驾驶行为谱系。不管是正常巡航、变道、避障，还是更少见的紧急刹车、紧急转向等行为，模型都通过一套相同的感知—理解—控制链条来处理，输出一组行为决策这种做法倾向于一般普通场景，但会有长尾问题。

## 文章主要贡献
### Vision MoE：动态视觉选择与融合
学习型路由器	根据当前驾驶上下文，从多个摄像头视图中选择与决策最相关的视角。

投影层（Projection Layer）	将不同视角转换为统一的特征空间，便于融合。

视图融合机制	将选定视角的信息整合为一个紧凑、连贯的视觉表示。

### Action MoE：多专家规划策略选择

专家网络（Experts）	每个专家负责一种特定的动作行为策略，如：车道跟随、避障、紧急制动、激进变道等。

路由机制（Routing Policy）	根据环境状态，在规划时动态选择/激活合适的专家，输出更适应情境的轨迹。

Flow-matching 框架	用于分布式轨迹建模与规划的高效方法，与专家模块协同工作。

`Flow Matching（流匹配） 是一种分布学习方法，本质上用于学习一个轨迹分布，使得模型可以从任意起点高效地生成一条自然、合理、上下文一致的动作轨迹（例如在自动驾驶中预测车辆的未来路径）（是生成轨迹而非指令）。`

## 详细流程
原始传感器：输入为多个摄像头的原始图像帧（RGB图像序列）；输出多摄像头视角的视觉特征Token（高维特征向量），作为Vision MoE的输入。

Vision MoE：输入为多摄像头视觉特征Token，驾驶场景上下文信息（如速度、地图等辅助状态）；作用为动态选择部分摄像头视角，过滤无关信息，投影成统一特征空间，融合成紧凑视觉Token序列；输出为融合后的“动态视角视觉Token”序列，代表当前关键视觉信息。

LLM：输入为Vision MoE输出的视觉Token序列 + 文本Token（任务指令、对话上下文等） + 车辆状态Token（速度、位置、加速度等） + 噪声Token（用于训练多样性）；作用为跨模态信息融合，建立视觉、语言、状态三者之间的深层联系，形成多模态语义理解；输出为多模态融合的高级语义特征Token序列，含丰富上下文和任务相关信息。

Action MoE：输入为预训练LLM输出的多模态语义特征Token + 当前驾驶环境状态；作用为根据上下文动态激活多个动作专家，每个专家专注于一种驾驶行为（如车道保持、紧急避障等）；输出为专家加权融合后的行为特征向量，用于指导后续轨迹规划。

Flow Matching：输入为Action MoE输出的行为特征向量 + 车辆当前状态信息；作用为基于向量场学习，生成连续且平滑的未来车辆轨迹；输出为车辆未来一段时间内的参考轨迹（位置、速度、航向序列）。

## 创新点细节
### 视觉MoE
![](https://pic3.zhimg.com/v2-0c43dbcdb77aba68cf98f4e319412224_r.jpg)
简单来说是根据前视图摄像头的嵌入向量和目标点来计算所有视图的选择概率，选取top-k，没被选中的就不输入到llm，节省了运算。后续大模型输入为固定视图token+动态视图token+文本token。每个视觉视图需要有位置嵌入（PE，是由学习来的），以保存不同视觉视图之间的空间和位置关系。标签是手动设计的滤波器基于未来轨迹、边界框和地图注释。最后使用交叉熵损失进行训练。

### 动作MoE
![](https://picx.zhimg.com/v2-fa9a6617bb5eba40f8bb0a95c14df7a5_r.jpg)
根据当前情境自然选择适当的驾驶技能，本文提出了基于原始流匹配轨迹变压器的 Skill-Specialized Action MoE 架构。在解码器中用包含多个技能专用专家的 Mixture-of-Experts (MoE) 层替换每个密集前馈网络（FFN），来分解策略的行为表示。

简单来说，通过Transformer的隐藏层状态，来计算各个非共享专家的概率分布，后续非共享专家输出加权，再加上共享专家模型的输出，构成更新后的隐藏状态。事件中可只使用几个专家进行计算，减少计算量，防止专家间互相干扰，增强专家的专业化程度。

使用交叉熵损失训练技能路由器，使用流匹配损失与技能路由器损失的和来训练整个动作MoE。引入正则化损失以保证专家利用率的平衡。还可以加入噪声，引入随机性。

### 训练过程
第一阶段，路由器不使用，专家的选择使用真实专家，同时训练路由器，稳定训练过程；后续使用路由器来选择输出专家，提高鲁棒性和泛化能力。

## 实验结果
### 实验细节
视觉路由标注（每个视图的重要性）、动作路由标注（五种驾驶技能）、Drive- $\pi_0$ （连续两个前视图作为输入，来估计周围agent速度，其他输入结合了当前和历史信息，包括位置、速度、加速度和航向角，以便于预测未来10个路径点）

`Drive- $\pi_0$ ：Drive-π0 的输入包括：
(i) 来自车载多视觉传感器的一系列环绕视图图像；
(ii) 一个固定的文本提示（例如，“请预测未来轨迹”）；
(iii) 当前车辆状态（例如，速度、偏航率和过去轨迹）。
网络设计遵循 π0 框架，采用预训练的 Paligemma VLM作为主干，并使用基于流匹配的动作模块生成规划的未来轨迹。 `

DriveMoE：使用连续两个前视图加上一个路由器动态选择的视觉视图作为输入。前者用于建模周围agent速度，hi偶这使用top-1视图来增强空间感知。输入状态模式与π0 框架保持一致。动作模型采用1个共享专家和6个非共享专家。选则动作的top-3专家用于最终的轨迹预测，包含10个未来路径点。
### 与SOTA对比
![](https://pic3.zhimg.com/v2-f7440e4853a0bba344b2fcdc5228f41a_r.jpg)
在 Bench2Drive 闭环基准的驾驶分数和成功率方面达到了最先进的 (SOTA) 性能。(开环指标主要作为模型收敛的指示器，而闭环指标更能可靠地评估真实驾驶性能。)
![](https://pic3.zhimg.com/v2-b4778bdcdada72f6b1a36e29af50698c_1440w.jpg)
文章在五个关键能力和整体平均值上均达到最先进的结果。

## 消融实验
### Drive-π0 与 DriveMoE 的对比
![](https://pic3.zhimg.com/v2-d44f0ecdb1036cb2d0fd6969d9a7848e_r.jpg)
MoE中，移除 Vision MoE 或 Action MoE 中的任何一个都会导致驾驶分数和成功率明显下降，表明每个组件对整体性能都有重要意义。
### 视觉MoE：视图选择和监督
![](https://pic1.zhimg.com/v2-3265f5952e7733a9c34300f0f0fba6ea_r.jpg)
添加视图、添加无监督动态选择和显式动态选择都会增强驾驶分数和成功率。
### 动作MoE：非共享专家数量
需要平衡专家数量，适当增加专家可以提高专业化水平，加的过多会存在负载不平衡，对性能产生负面影响，也会相互干扰。

