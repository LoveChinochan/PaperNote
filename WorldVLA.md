# WorldVLA:世界模型实现视觉-动作双向增强，抓取精度显著提升
## 简要介绍
WorldVLA是一种将动作与图像理解和生成相结合的自回归动作世界模型。集成了VLA和世界模型。通过动作和图像理解来预测未来图像，旨在学习环境的底层物理规律来改进动作生成。动作模型基于图像观察生成后续动作，辅助视觉理解，帮助世界模型的视觉生成。

本文的WorldVLA，二者结合的性能较单独的动作模型和世界模型要更优，突显出世界模型与动作模型之间的相互增强的作用。

以自回归方式生成动作序列时，动作模型的性能会下降，归因于模型泛化能力有限，导致早期动作的误差传播到后续动作。本文采用注意力掩码策略，在生成当前动作时选择性地掩码先前动作，有了显著性的性能提升。

![](https://pic2.zhimg.com/v2-594de454510707939eb7c508d4d147c7_r.jpg)
本文的方法实现了前两个的结合。较动作模型多了对图像的生成和行为的理解，较世界模型多了动作的生成。

 自回归顺序生成动作会导致性能下降。这是预训练的多模态语言模型主要是以图像和文本，导致应用在生成动作上时，泛化能力弱。使用选择性的掩码策略，掩盖住一部分先前动作，减少误差累积，在动作块的生成任务中产生了显著的性能提升。

`动作模型负责基于图像观察历史和语言指令生成动作;世界模型基于观察历史序列和相应的动作序列预测下一帧 `

 ## 主要贡献
 一个模型： WorldVLA，将动作和图像理解与生成相结合的自回归动作世界模型
 
 一种掩码策略：为自回归生成动作块时，加入一种动作注意力掩码策略，解决了顺序生成多个动作时的误差累积

 优异的实验结果：优于独立的动作与世界模型；掩码策略解决了性能退化问题，并显著提高了抓取性能。

 ## 实验方法
 ![](https://pic4.zhimg.com/v2-17f1ff7ba0b4f33a7a11a8aeb5fc81c3_1440w.jpg)
 该模型的主要结构如上，分为两大部分。动作模型接受任务指令和当前及历史观察状态，生成动作块。将动作块的输出以及规定好的语言指令以及当前及历史观察状态输入进世界模型，生成未来图像帧。

 ### 涉及三个tokenizer
 图像tokenizer：是一个VQ-GAN模型，带有对特定图像区域（面部和显著对象）的额外感知损失。压缩比为16，codebook大小为8192。

 `对特定图像区域的额外感知损失，可以加强对关键区域的还原质量`

 动作tokenizer：使用分箱操作。每个维度都分成256箱，箱宽度由训练数据范围决定。一个动作包含7个token，有3个性相对位置、3个相对角度和1个绝对抓取器状态。

 语言tokenizer：采用训练过的BPE（Byte Pair Encoding，可合并高频词，提高泛化能力，编码速度快，词表大小可指定）词汇量65536，包含8192个图像token和256个动作token。

 ### 训练策略
 引入世界模型增强动作生成，主要原因有：

 世界模型从当前状态和应用的动作预测未来的图像，来获得对环境的物理解释，对后续操纵任务有帮助；

 世界模型使系统能够模拟和评估动作的潜在结果，有利于避免可能导致不利状态的动作；

 世界模型需要对动作的输入做出精确解释，支持了动作模型可以产生更有效和符合上下文的动作。

 动作模型通过以上增强了视觉理解，进而支持了世界模型的视觉生成能力。

### 两个模型的输入输出数据结构
#### 动作模型
包含起始、结束的token，以及文本、图像和动作token。输入是文本和图像，输出动作，只需计算动作token的损失。
#### 世界模型
无需任务指令（任务是固定，预测下一帧图像，动作本身可以完全确定下一状态）。基于动作的下一帧预测重复N次，输出是图像，只需计算图像token的损失。

### 训练目标
损失函数为动作模型数据和世界模型数据的交叉熵损失之和。使用缩放因子来平衡损失贡献，因为图像token数远多于动作token。 $L = L_{action} + \alpha L_{world}$

### 注意力掩码
![](https://pic2.zhimg.com/v2-eb18cf431866c4203ce3e13ad6680f4d_r.jpg)
传统的采用因果注意力掩码，该掩码限制当前token只能访问来自先前token的信息，排除任何后续token。由于MLLM在数据集上较少接触动作，故而泛化能力有限。在默认因果注意力掩码下，早期的动作误差会传播到后续动作。本文专门为动作又设计了掩码，修改后的掩码确保当前动作仅依赖于文本和视觉输入，同时禁止访问先前动作。

## 实验结果
使用LIBERO基准（包含4个子任务）。

